{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "Same concept as simple linear regression, estimating the relationship between independent and dependent variables, but with multiple independent variables (x-variables).\n",
    "\n",
    "### Formula:\n",
    "y = b0 + b1\\*x1 + b2\\*x2 + bn*xn\n",
    "* 'n' is the number of x-variables\n",
    "\n",
    "### Assumptions of Linear Regression\n",
    "All these assumptions must be **true** before building a linear regression model\n",
    "1. Linearity\n",
    "2. Homoscedasticity\n",
    "3. Multivariate Normality\n",
    "4. Independence of Errors\n",
    "5. Lack of Multicollinearity\n",
    "\n",
    "### 3D Visual Representation\n",
    "\n",
    "<img src=\"images/example_mlr.png\" width=\"600\" height=\"600\" align=\"center\"/>\n",
    "\n",
    "https://towardsdatascience.com/graphs-and-ml-multiple-linear-regression-c6920a1f2e70\n",
    "\n",
    "* 'Weight' & 'Horsepower' are the two independent variables\n",
    "    * These variables are independent of each otehr\n",
    "* 'MPG' is the dependent variable\n",
    "\n",
    "As a result of the multiple linear regression, a hyper-plane is created in 3D space.\n",
    "* Hyper-plane is created through the Ordinary Least Squares (OLS) method\n",
    "\n",
    "### Feature Scaling? Not Needed in Regression!\n",
    "Feature scaling is not needed for even simple or multiple linear regression models since the dependent variable is a combination of the independent variables, sot the coeefficients (slopes) of each independent variable would adopt a scale to put everything on the same scale.\n",
    "\n",
    "For example, y = 0.5x. If y = 1000 when x = 2000, then the coefficient (slope) equals 0.5 to scale the result properly. This same concept applies in multi-variate linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Variables\n",
    "Sometimes variables are non-numeric and show categorical information. These variables/columns would require dummy variables to use within a mathematical equation like Multiple Linear Regression.\n",
    "\n",
    "<img src=\"images/dummy_variable_table.png\" width=\"600\" height=\"600\" align=\"center\"/>\n",
    "\n",
    "In the example above, the \"State\" column is a categorical variable, thus it needs dummy variables.\n",
    "* For each unique value in the State column, create a Dummy variable/column that indicates if the category was present or not present at the row\n",
    "\n",
    "### Dummy Variable Trap\n",
    "In the example, we crossed-out the \"California\" dummy variable. But why? \n",
    "\n",
    "In a multi-variate model, it's necessary to always omit one dummy variable.\n",
    "* This is because there's always a remaining duplicate variable\n",
    "\n",
    "The dummy variable trap causes perfect multicollinearity (highly correlated variables) which skews the regression model. Perfect multicollinearity is when a variable can determine the value of another variable, so one predictor variable can be used to predict another. This creates redundancy which skews the results of a regression model.\n",
    "\n",
    "#### Handling Dummy Variable Trap\n",
    "Fortunately, SKLearn automatically removes highly correlated variables (by calculating correlation coefficients), so the dummy variable trap would be automatically resolved by SKLearn.\n",
    "\n",
    "However, other statistical models (such as the statsmodels API) may not handle the trap. To handle the trap, just omit a single dummy variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Omitting Variables (Model Building)\n",
    "Sometimes variables must be thrown-out because they're \"garbage-in\" data that would lead to \"garbage-out\" predictions.\n",
    "\n",
    "We don't want this garbage data because they might not predict anything useful for the model.\n",
    "\n",
    "### P-Values\n",
    "The p-value tells us how likely it is to get a result like this if the Null Hypothesis is true.\n",
    "\n",
    "P-values determine if an outcome is statistically significant.\n",
    "* **p-value <= 0.05** is **statistically significant:**\n",
    "    * **If p <= 0.05**, then we reject the null hypothesis **(statistically significant).**\n",
    "    * **If p >= 0.05**, then we accept null hypothesis **(not statistically significant).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Methods of Building Models\n",
    "**1. All-in**\n",
    "* Use all the variables\n",
    "\n",
    "**2. Backward Elimination**\n",
    "* Step 1: Select a significance level (0.05) to stay in the model\n",
    "* Step 2: Fit the full model with all possible predictors (variables) from Step 1\n",
    "* Step 3: Consider the predictor with the highest P-value\n",
    "    * If p-value > 0.05, continue to Step 4\n",
    "    * If p-value <= 0.05, the model is ready\n",
    "* Step 4: Remove the predictor and re-fit the model without the predictor. Then go to Step 3.\n",
    "\n",
    "**3. Forward Selection**\n",
    "* Step 1: Select a significance level (0.05) to enter and stay in the model\n",
    "* Step 2: Fit ALL possible regression models, y ~ xn. Select the one with the lowest p-value\n",
    "* Step 3: Keep this variable, then fit all possible models an extra predictor added to the one(s) you kept\n",
    "* Step 4: Consider the predictor with the lowest P-value\n",
    "    * If p-value <= 0.05, go to Step 3\n",
    "    * If p-value > 0.05, the model is ready\n",
    "    \n",
    "**4. Bidirectional Elimination**\n",
    "* Step 1: Select a significance level (0.05) to enter and to stay in the model\n",
    "* Step 2: Perform the next step of Forward Selection\n",
    "    * New variables must have: p-value <= 0.05 to enter)\n",
    "* Step 3: Perform all steps of Backward Elimination \n",
    "    * Old variables must have p-value <= 0.05 to stay)\n",
    "* Step 4: No new variables can enter and no old variables can exit. The model is ready.\n",
    "\n",
    "**5. Score Comparison (All Possible Models)**\n",
    "* Step 1: Select a criterion of goodness of fit (i.e. Akaike Criterion)\n",
    "* Step 2: Construct all possible regression models\n",
    "    * 2^n - 1 total combinations\n",
    "* Step 3: Select the one with the best criterion. Then your model is ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLearn Multi-Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***IMPORTING LIBRARIES***\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***IMPORTING THE DATASET***\n",
    "data = pd.read_csv('data/50_Startups.csv')\n",
    "x = data.iloc[:,:-1].values\n",
    "y = data.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***ENCODING CATEGORICAL DATA (INDEPENDENT VARIABLES)***\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# [3] is the index of the 'State' column\n",
    "dummy_transformer = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [3])], remainder='passthrough')\n",
    "x = np.array(dummy_transformer.fit_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***SPLITTING THE DATASET INTO THE TRAINING AND TEST SET***\n",
    "from sklearn.model_selection import train_test_split \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#***TRAINING THE MULTIPLE LINEAR REGRESSION MODEL ON THE TRAINING SET***\n",
    "# import the LinearRegression() class\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# create a regressor model\n",
    "regressor = LinearRegression()\n",
    "\n",
    "# fit the training data, feature scaling is not needed for regression models\n",
    "regressor.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[103015.2  103282.38]\n",
      " [132582.28 144259.4 ]\n",
      " [132447.74 146121.95]\n",
      " [ 71976.1   77798.83]\n",
      " [178537.48 191050.39]\n",
      " [116161.24 105008.31]\n",
      " [ 67851.69  81229.06]\n",
      " [ 98791.73  97483.56]\n",
      " [113969.44 110352.25]\n",
      " [167921.07 166187.94]]\n"
     ]
    }
   ],
   "source": [
    "#***PREDICTING THE TEST SET RESULTS***\n",
    "# the vector of the predicted profit in the test set\n",
    "y_pred = regressor.predict(x_test)\n",
    "\n",
    "# formats the output to 2 decimal places\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# predicted vs actual test results\n",
    "comparison = np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)), 1)\n",
    "\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 103015.2 vs Actual: 103282.38 ---> Difference: 267.18\n",
      "Predicted: 132582.28 vs Actual: 144259.4 ---> Difference: 11677.12\n",
      "Predicted: 132447.74 vs Actual: 146121.95 ---> Difference: 13674.21\n",
      "Predicted: 71976.1 vs Actual: 77798.83 ---> Difference: 5822.73\n",
      "Predicted: 178537.48 vs Actual: 191050.39 ---> Difference: 12512.91\n",
      "Predicted: 116161.24 vs Actual: 105008.31 ---> Difference: 11152.93\n",
      "Predicted: 67851.69 vs Actual: 81229.06 ---> Difference: 13377.37\n",
      "Predicted: 98791.73 vs Actual: 97483.56 ---> Difference: 1308.17\n",
      "Predicted: 113969.44 vs Actual: 110352.25 ---> Difference: 3617.19\n",
      "Predicted: 167921.07 vs Actual: 166187.94 ---> Difference: 1733.13\n"
     ]
    }
   ],
   "source": [
    "# compare y_pred (prediction) to the y_test (actual)\n",
    "i = 0\n",
    "while i < len(y_pred):\n",
    "    diff = abs(round(y_pred[i], 2) - y_test[i])\n",
    "    print(\"Predicted: \" + str(round(y_pred[i], 2)) + \" vs Actual: \" + str(round(y_test[i], 2)) +\n",
    "          \" ---> Difference: \" + str(round(diff, 2)))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a Single Prediction\n",
    "For example the profit of a startup with R&D Spend = 160000, Administration Spend = 130000, Marketing Spend = 300000 and State = 'California'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[181566.92]\n"
     ]
    }
   ],
   "source": [
    "print(regressor.predict([[1, 0, 0, 160000, 130000, 300000]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, our model predicts that the profit of a Californian startup which spent 160000 in R&D, 130000 in Administration and 300000 in Marketing is $ 181566,92.\n",
    "\n",
    "**Important note 1:** Notice that the values of the features were all input in a double pair of square brackets. That's because the \"predict\" method always expects a 2D array as the format of its inputs. And putting our values into a double pair of square brackets makes the input exactly a 2D array. Simply put:\n",
    "\n",
    "$1, 0, 0, 160000, 130000, 300000 \\rightarrow \\textrm{scalars}$\n",
    "\n",
    "$[1, 0, 0, 160000, 130000, 300000] \\rightarrow \\textrm{1D array}$\n",
    "\n",
    "$[[1, 0, 0, 160000, 130000, 300000]] \\rightarrow \\textrm{2D array}$\n",
    "\n",
    "**Important note 2:** Notice also that the \"California\" state was not input as a string in the last column but as \"1, 0, 0\" in the first three columns. That's because of course the predict method expects the one-hot-encoded values of the state, and as we see in the second row of the matrix of features X, \"California\" was encoded as \"1, 0, 0\". And be careful to include these values in the first three columns, not the last three ones, because the dummy variables are always created in the first columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Final Linear Regression Equation With the Values of the Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressor Coefficients:  [ 8.66e+01 -8.73e+02  7.86e+02  7.73e-01  3.29e-02  3.66e-02]\n",
      "Regressor Intercept:  42467.52924854249\n"
     ]
    }
   ],
   "source": [
    "print('Regressor Coefficients: ', regressor.coef_)\n",
    "print('Regressor Intercept: ', regressor.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the equation of our multiple linear regression model is:\n",
    "\n",
    "$$\\textrm{Profit} = 86.6 \\times \\textrm{Dummy State 1} - 873 \\times \\textrm{Dummy State 2} + 786 \\times \\textrm{Dummy State 3} - 0.773 \\times \\textrm{R&D Spend} + 0.0329 \\times \\textrm{Administration} + 0.0366 \\times \\textrm{Marketing Spend} + 42467.53$$\n",
    "\n",
    "**Important Note:** To get these coefficients we called the \"coef_\" and \"intercept_\" attributes from our regressor object. Attributes in Python are different than methods and usually return a simple value or an array of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression - Backward Elimination\n",
    "There might've been garbage variables that scewed the predictions. We can attempt to remove these garbage variables through a backward elimination process.\n",
    "\n",
    "https://www.geeksforgeeks.org/ml-multiple-linear-regression-backward-elimination-technique/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a stats model (sm)\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "add a column of 1s as the first column of x.\n",
    "this column is represented as b0 for the stats model to perform the backward elimination.\n",
    "\"\"\"\n",
    "fifty_ones = np.ones((50, 1)).astype(int)\n",
    "x = np.append(fifty_ones, x, axis=1)\n",
    "\n",
    "# deep copy x onto a x_opt variable, which will contain the significant independent variables\n",
    "x_opt =  np.array(x[:, [0, 1, 2, 3, 4, 5]], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.943</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   205.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 20 Jul 2020</td> <th>  Prob (F-statistic):</th> <td>2.90e-28</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:20:06</td>     <th>  Log-Likelihood:    </th> <td> -526.75</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1064.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    45</td>      <th>  BIC:               </th> <td>   1073.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 4.122e+04</td> <td> 4607.941</td> <td>    8.945</td> <td> 0.000</td> <td> 3.19e+04</td> <td> 5.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td> 1.339e+04</td> <td> 2421.500</td> <td>    5.529</td> <td> 0.000</td> <td> 8511.111</td> <td> 1.83e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td> 1.448e+04</td> <td> 2518.987</td> <td>    5.748</td> <td> 0.000</td> <td> 9405.870</td> <td> 1.96e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td> 1.335e+04</td> <td> 2459.306</td> <td>    5.428</td> <td> 0.000</td> <td> 8395.623</td> <td> 1.83e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    0.8609</td> <td>    0.031</td> <td>   27.665</td> <td> 0.000</td> <td>    0.798</td> <td>    0.924</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>   -0.0527</td> <td>    0.050</td> <td>   -1.045</td> <td> 0.301</td> <td>   -0.154</td> <td>    0.049</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.275</td> <th>  Durbin-Watson:     </th> <td>   1.197</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  19.260</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.953</td> <th>  Prob(JB):          </th> <td>6.57e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.369</td> <th>  Cond. No.          </th> <td>9.16e+17</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 1.29e-24. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.948\n",
       "Model:                            OLS   Adj. R-squared:                  0.943\n",
       "Method:                 Least Squares   F-statistic:                     205.0\n",
       "Date:                Mon, 20 Jul 2020   Prob (F-statistic):           2.90e-28\n",
       "Time:                        22:20:06   Log-Likelihood:                -526.75\n",
       "No. Observations:                  50   AIC:                             1064.\n",
       "Df Residuals:                      45   BIC:                             1073.\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       4.122e+04   4607.941      8.945      0.000    3.19e+04    5.05e+04\n",
       "x1          1.339e+04   2421.500      5.529      0.000    8511.111    1.83e+04\n",
       "x2          1.448e+04   2518.987      5.748      0.000    9405.870    1.96e+04\n",
       "x3          1.335e+04   2459.306      5.428      0.000    8395.623    1.83e+04\n",
       "x4             0.8609      0.031     27.665      0.000       0.798       0.924\n",
       "x5            -0.0527      0.050     -1.045      0.301      -0.154       0.049\n",
       "==============================================================================\n",
       "Omnibus:                       14.275   Durbin-Watson:                   1.197\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               19.260\n",
       "Skew:                          -0.953   Prob(JB):                     6.57e-05\n",
       "Kurtosis:                       5.369   Cond. No.                     9.16e+17\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 1.29e-24. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an OLS regressor to fit the model with all possible predictors\n",
    "regressor_OLS = sm.OLS(endog=y, exog=x_opt).fit()\n",
    "\n",
    "\"\"\"\n",
    "We remove x5 since it is the highest valued parameter\n",
    "\"\"\"\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.947</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.943</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   272.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 20 Jul 2020</td> <th>  Prob (F-statistic):</th> <td>2.76e-29</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:30:31</td>     <th>  Log-Likelihood:    </th> <td> -527.35</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1063.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    46</td>      <th>  BIC:               </th> <td>   1070.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 3.686e+04</td> <td> 1959.786</td> <td>   18.806</td> <td> 0.000</td> <td> 3.29e+04</td> <td> 4.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td> 1.189e+04</td> <td> 1956.677</td> <td>    6.079</td> <td> 0.000</td> <td> 7955.697</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td> 1.306e+04</td> <td> 2122.665</td> <td>    6.152</td> <td> 0.000</td> <td> 8785.448</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>  1.19e+04</td> <td> 2036.022</td> <td>    5.847</td> <td> 0.000</td> <td> 7805.580</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    0.8530</td> <td>    0.030</td> <td>   28.226</td> <td> 0.000</td> <td>    0.792</td> <td>    0.914</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>13.418</td> <th>  Durbin-Watson:     </th> <td>   1.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  17.605</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.907</td> <th>  Prob(JB):          </th> <td>0.000150</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.271</td> <th>  Cond. No.          </th> <td>3.70e+17</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 2.74e-24. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.947\n",
       "Model:                            OLS   Adj. R-squared:                  0.943\n",
       "Method:                 Least Squares   F-statistic:                     272.4\n",
       "Date:                Mon, 20 Jul 2020   Prob (F-statistic):           2.76e-29\n",
       "Time:                        22:30:31   Log-Likelihood:                -527.35\n",
       "No. Observations:                  50   AIC:                             1063.\n",
       "Df Residuals:                      46   BIC:                             1070.\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       3.686e+04   1959.786     18.806      0.000    3.29e+04    4.08e+04\n",
       "x1          1.189e+04   1956.677      6.079      0.000    7955.697    1.58e+04\n",
       "x2          1.306e+04   2122.665      6.152      0.000    8785.448    1.73e+04\n",
       "x3           1.19e+04   2036.022      5.847      0.000    7805.580     1.6e+04\n",
       "x4             0.8530      0.030     28.226      0.000       0.792       0.914\n",
       "==============================================================================\n",
       "Omnibus:                       13.418   Durbin-Watson:                   1.122\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               17.605\n",
       "Skew:                          -0.907   Prob(JB):                     0.000150\n",
       "Kurtosis:                       5.271   Cond. No.                     3.70e+17\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 2.74e-24. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repeat: re-create an OLS regressor to fit the model with all possible predictors\n",
    "x_opt =  np.array(x[:, [0, 1, 2, 3, 4]], dtype=float)\n",
    "regressor_OLS = sm.OLS(endog=y, exog=x_opt).fit()\n",
    "\n",
    "\"\"\"\n",
    "Based on the summary results, there are no more columns with a p-value greater than 0.05.\n",
    "Therefore, we finally got the significant columns among the independent variables!\n",
    "\"\"\"\n",
    "regressor_OLS.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
